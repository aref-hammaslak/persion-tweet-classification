{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Classification in Persian tweets \n",
    "## Using CAR and Multinomial Naive Bayes Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Importing Required Libraries\n",
    "This cell imports all necessary libraries for:\n",
    "- Text processing (hazm for Persian NLP)\n",
    "- Data manipulation (pandas, numpy)\n",
    "- Machine learning (sklearn)\n",
    "- Model evaluation and preprocessing\n",
    "- File handling and system operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import hazm\n",
    "from hazm import Stemmer, Normalizer, Lemmatizer, word_tokenize\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing Setup\n",
    "Initializes Persian NLP tools and defines preprocessing functions:\n",
    "- `preprocess_text()`: Handles text normalization and cleaning\n",
    "- `extract_tokens()`: Performs tokenization, lemmatization, and stemming\n",
    "- Removes stopwords and short tokens\n",
    "- Returns processed text ready for vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLP tools\n",
    "stemmer = Stemmer()\n",
    "stopwords = hazm.stopwords_list()\n",
    "normalizer = Normalizer()\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = normalizer.normalize(text)                 # Normalize first\n",
    "    text = text.lower()                               # Then lowercase\n",
    "    text = re.sub(r'[^آ-ی\\s]', '', text)               # Remove non-Persian chars and '#' characters\n",
    "    return text\n",
    "\n",
    "def extract_tokens(text):\n",
    "    text = preprocess_text(text)\n",
    "    tokens = word_tokenize(text)                      # Tokenize\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize\n",
    "    tokens = [stemmer.stem(token) for token in tokens]          # Optionally stem\n",
    "    tokens = [token for token in tokens if token not in stopwords]  # Remove stopwords\n",
    "    tokens = [token for token in tokens if len(token) > 2]  # Remove short tokens\n",
    "    return ' '.join(tokens)  # Return as space-separated string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading and Initial Processing\n",
    "- Sets up directory structure for dataset and vocabulary\n",
    "- Processes each emotion category (anger, fear, joy, sad, disgust, surprise)\n",
    "- Applies text preprocessing to all tweets\n",
    "- Displays initial dataset distribution showing class imbalance\n",
    "- Key observation: Significant imbalance between classes (e.g., 34,328 'sad' vs 925 'disgust')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing anger category\n",
      "Processing fear category\n",
      "Processing joy category\n",
      "Processing sad category\n",
      "Processing disgust category\n",
      "Processing surprise category\n",
      "\n",
      "Dataset distribution:\n",
      "label\n",
      "sad         34328\n",
      "joy         28024\n",
      "anger       20069\n",
      "fear        17624\n",
      "surprise    12859\n",
      "disgust       925\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set up directories\n",
    "dataset_dir = (pathlib.Path().absolute() / '../dataset').resolve()\n",
    "vocab_dir = (pathlib.Path().absolute() / '../vocab').resolve()\n",
    "vocab_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Process dataset\n",
    "dataset_df = pd.DataFrame()\n",
    "categories = ['anger', 'fear', 'joy', 'sad', 'disgust', 'surprise']\n",
    "\n",
    "for category in categories:\n",
    "    print(f\"Processing {category} category\")\n",
    "    file = dataset_dir / 'raw' / f\"{category}.csv\"\n",
    "    df = pd.read_csv(file)\n",
    "    df['label'] = category\n",
    "    df['processed_text'] = df['tweet'].apply(extract_tokens)\n",
    "    dataset_df = pd.concat([dataset_df, df], axis=0)\n",
    "\n",
    "# Print dataset statistics\n",
    "print(\"\\nDataset distribution:\")\n",
    "print(dataset_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes Implementation\n",
    "- Initializes TF-IDF vectorizer with optimized parameters:\n",
    "  - 10,000 max features\n",
    "  - Minimum document frequency of 5\n",
    "  - Maximum document frequency of 95%\n",
    "  - Uses both unigrams and bigrams\n",
    "- Transforms text data into TF-IDF features\n",
    "- Prepares labels for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize TF-IDF vectorizer with limited vocabulary size\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10000,  # Limit vocabulary to top 10,000 words\n",
    "    min_df=5,           # Ignore terms that appear in less than 5 documents\n",
    "    max_df=0.95,        # Ignore terms that appear in more than 95% of documents\n",
    "    ngram_range=(1, 2)  # Use both unigrams and bigrams\n",
    ")\n",
    "\n",
    "# Transform text to TF-IDF features\n",
    "X = tfidf.fit_transform(dataset_df['processed_text'])\n",
    "y = dataset_df['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Balancing and Splitting\n",
    "- Applies RandomOverSampler to address class imbalance:\n",
    "  - Creates synthetic samples for minority classes\n",
    "  - Ensures equal representation of all emotions\n",
    "  - Helps prevent model bias towards majority classes\n",
    "- Splits the balanced dataset:\n",
    "  - 80% for training\n",
    "  - 20% for testing\n",
    "  - Uses random state for reproducibility\n",
    "- This step is crucial for:\n",
    "  - Fair model evaluation\n",
    "  - Better learning of minority class patterns\n",
    "  - More robust performance across all emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample minority classes\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Split dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation (MNB)\n",
    "- Trains Multinomial Naive Bayes classifier\n",
    "- Makes predictions on test set\n",
    "- Evaluates model performance:\n",
    "  - Overall accuracy: 81%\n",
    "  - Detailed classification report for each emotion\n",
    "- Saves model and vectorizer for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy: 0.81\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.79      0.77      0.78      6884\n",
      "     disgust       0.82      0.96      0.88      6945\n",
      "        fear       0.86      0.76      0.81      6898\n",
      "         joy       0.82      0.76      0.78      6836\n",
      "         sad       0.77      0.75      0.76      6850\n",
      "    surprise       0.80      0.86      0.83      6781\n",
      "\n",
      "    accuracy                           0.81     41194\n",
      "   macro avg       0.81      0.81      0.81     41194\n",
      "weighted avg       0.81      0.81      0.81     41194\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/aref/projects/uni-related/hashtag-suggestion/models/tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Train model\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the model and vectorizer for later use\n",
    "import joblib\n",
    "model_dir = (pathlib.Path().absolute() / '../models').resolve()\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "joblib.dump(model, model_dir / 'emotion_classifier.joblib')\n",
    "joblib.dump(tfidf, model_dir / 'tfidf_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAR Model Implementation\n",
    "- Applies feature scaling using StandardScaler\n",
    "- Calculates class weights to handle imbalance\n",
    "- Implements CAR using LogisticRegression with:\n",
    "  - Adaptive regularization (C=1.0)\n",
    "  - Class weights for imbalance handling\n",
    "  - LBFGS solver for optimization\n",
    "  - Multinomial approach for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CAR Model Accuracy: 0.90\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.89      0.89      0.89      6884\n",
      "     disgust       0.98      1.00      0.99      6945\n",
      "        fear       0.89      0.93      0.91      6898\n",
      "         joy       0.88      0.84      0.86      6836\n",
      "         sad       0.86      0.79      0.82      6850\n",
      "    surprise       0.93      0.97      0.95      6781\n",
      "\n",
      "    accuracy                           0.90     41194\n",
      "   macro avg       0.90      0.90      0.90     41194\n",
      "weighted avg       0.90      0.90      0.90     41194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scaler = StandardScaler(with_mean=False) \n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = dict(zip(np.unique(y_train), \n",
    "                        [len(y_train) / (len(np.unique(y_train)) * np.sum(y_train == c)) \n",
    "                         for c in np.unique(y_train)]))\n",
    "\n",
    "# Train CAR model\n",
    "car_model = LogisticRegression(\n",
    "    C=1.0,  # Regularization strength\n",
    "    class_weight=class_weights,\n",
    "    max_iter=1000,\n",
    "    solver='lbfgs',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "car_model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = car_model.predict(x_test_scaled)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nCAR Model Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Overall Metrics\n",
    "| Metric | Multinomial Naive Bayes | CAR Model |\n",
    "|--------|------------------------|-----------|\n",
    "| Accuracy | 81% | 90% |\n",
    "| Macro Avg F1 | 0.81 | 0.90 |\n",
    "| Weighted Avg F1 | 0.81 | 0.90 |\n",
    "\n",
    "### Class-wise Performance\n",
    "| Emotion | MNB F1 | CAR F1 | Improvement |\n",
    "|---------|--------|--------|-------------|\n",
    "| Anger | 0.78 | 0.89 | +0.11 |\n",
    "| Disgust | 0.88 | 0.99 | +0.11 |\n",
    "| Fear | 0.81 | 0.91 | +0.10 |\n",
    "| Joy | 0.78 | 0.86 | +0.08 |\n",
    "| Sad | 0.76 | 0.82 | +0.06 |\n",
    "| Surprise | 0.83 | 0.95 | +0.12 |\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "### 1. Performance Improvements\n",
    "- CAR model shows consistent improvement across all emotion categories\n",
    "- Most significant improvements in:\n",
    "  - Surprise classification (+0.12 F1)\n",
    "  - Disgust classification (+0.11 F1)\n",
    "  - Anger classification (+0.11 F1)\n",
    "- More balanced performance across all classes\n",
    "\n",
    "### 2. Technical Advantages\n",
    "- Better handling of class imbalance\n",
    "- More sophisticated feature space modeling\n",
    "- Improved regularization and optimization\n",
    "- Better capture of complex word relationships in Persian text\n",
    "\n",
    "### 3. Practical Implications\n",
    "- More reliable emotion classification\n",
    "- Better performance on minority classes\n",
    "- More balanced predictions across all emotions\n",
    "- Reduced bias towards majority classes\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The CAR model demonstrates superior performance in Persian emotion classification compared to Multinomial Naive Bayes. The 9% improvement in overall accuracy and more balanced performance across all emotion categories make it a more suitable choice for this task. The model's ability to handle class imbalance and capture complex feature interactions is particularly valuable for Persian text analysis, where word relationships and context play crucial roles in emotion expression.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
